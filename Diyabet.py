# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarÄ±yoruz

# Veri iÅŸleme ve analiz iÃ§in pandas
import pandas as pd 
# pandas, veri analizi ve manipÃ¼lasyonu iÃ§in kullanÄ±lan gÃ¼Ã§lÃ¼ bir kÃ¼tÃ¼phanedir.
# Veri setlerini DataFrame formatÄ±nda iÅŸler ve Ã¼zerinde kolayca iÅŸlemler yapÄ±lmasÄ±nÄ± saÄŸlar.

# SayÄ±sal iÅŸlemler iÃ§in numpy
import numpy as np 
# numpy, bÃ¼yÃ¼k veri setleri Ã¼zerinde yÃ¼ksek performanslÄ± matematiksel iÅŸlemler yapmamÄ±zÄ± saÄŸlar.
# Diziler (array) ile Ã§alÄ±ÅŸmak iÃ§in kullanÄ±lÄ±r ve vektÃ¶rel iÅŸlemleri kolaylaÅŸtÄ±rÄ±r.

# Grafik Ã§izimi iÃ§in matplotlib
import matplotlib.pyplot as plt 
# matplotlib, grafik Ã§izimi iÃ§in kullanÄ±lan popÃ¼ler bir kÃ¼tÃ¼phanedir.
# Veri analizinin gÃ¶rselleÅŸtirilmesini saÄŸlar, histogram, Ã§izgi grafiÄŸi gibi birÃ§ok grafik tÃ¼rÃ¼ oluÅŸturabiliriz.

# GeliÅŸmiÅŸ gÃ¶rselleÅŸtirme iÃ§in seaborn
import seaborn as sns 
# seaborn, matplotlib'in Ã¼zerine inÅŸa edilmiÅŸ, daha geliÅŸmiÅŸ ve estetik grafikler oluÅŸturmaya yardÄ±mcÄ± olan bir kÃ¼tÃ¼phanedir.
# Veri setlerinin daÄŸÄ±lÄ±mÄ±nÄ± ve iliÅŸkilerini daha kolay analiz etmemizi saÄŸlar.

# Veri standardizasyonu iÃ§in StandardScaler
from sklearn.preprocessing import StandardScaler 
# StandardScaler, veriyi standart normal daÄŸÄ±lÄ±ma dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in kullanÄ±lÄ±r.
# Verinin ortalamasÄ±nÄ± 0'a, standart sapmasÄ±nÄ± 1'e getirerek Ã¶lÃ§eklendirme yapar.
# Bu, Ã¶zellikle mesafe tabanlÄ± algoritmalar (KNN, SVM) ve Gradient Descent tabanlÄ± algoritmalar iÃ§in bÃ¼yÃ¼k Ã¶nem taÅŸÄ±r.
# Ã–lÃ§eklendirilmemiÅŸ veriler, modelin yanlÄ±ÅŸ Ã¶ÄŸrenmesine veya bazÄ± deÄŸiÅŸkenlerin diÄŸerlerine baskÄ±n olmasÄ±na neden olabilir.

# Veri setini eÄŸitim ve test kÃ¼melerine ayÄ±rmak iÃ§in train_test_split
# Model deÄŸerlendirmesi iÃ§in KFold, cross_val_score, GridSearchCV
from sklearn.model_selection import train_test_split , KFold, cross_val_score, GridSearchCV 
# train_test_split: Veriyi eÄŸitim ve test kÃ¼melerine ayÄ±rarak modelin performansÄ±nÄ± Ã¶lÃ§memizi saÄŸlar.
# KFold: K-katlÄ± Ã§apraz doÄŸrulama yÃ¶ntemiyle modeli farklÄ± veri bÃ¶lÃ¼mlerinde test etmemize olanak tanÄ±r.
# cross_val_score: Modelin Ã§apraz doÄŸrulama ile deÄŸerlendirilmesini saÄŸlar.
# GridSearchCV: Hiperparametre optimizasyonu yaparak en iyi model parametrelerini belirler.

# SÄ±nÄ±flandÄ±rma modellerinin performansÄ±nÄ± deÄŸerlendirmek iÃ§in classification_report ve confusion_matrix
from sklearn.metrics import classification_report, confusion_matrix 
# classification_report: Modelin doÄŸruluk, hassasiyet, f1 skoru gibi performans metriklerini detaylÄ± ÅŸekilde verir.
# confusion_matrix: Modelin tahmin ettiÄŸi ve gerÃ§ek deÄŸerler arasÄ±ndaki iliÅŸkiyi gÃ¶steren hata matrisi oluÅŸturur.

# Lojistik regresyon modeli
from sklearn.linear_model import LogisticRegression 
# LogisticRegression: Ä°kili sÄ±nÄ±flandÄ±rma problemleri iÃ§in kullanÄ±lan istatistiksel bir modeldir.
# OlasÄ±lÄ±k temelli Ã§alÄ±ÅŸarak verileri sÄ±nÄ±flandÄ±rÄ±r ve sigmoid fonksiyonu ile Ã§Ä±ktÄ± Ã¼retir.

# Karar aÄŸacÄ± sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±
from sklearn.tree import DecisionTreeClassifier 
# DecisionTreeClassifier: Veri seti iÃ§indeki Ã¶zelliklere dayalÄ± kararlar vererek sÄ±nÄ±flandÄ±rma yapan bir algoritmadÄ±r.
# AÄŸaÃ§ yapÄ±sÄ± oluÅŸturarak verileri dallara ayÄ±rÄ±r ve tahminler yapar.

# K-En YakÄ±n KomÅŸu (KNN) algoritmasÄ±
from sklearn.neighbors import KNeighborsClassifier 
# KNeighborsClassifier: Verinin en yakÄ±n K komÅŸusuna bakarak sÄ±nÄ±flandÄ±rma yapar.
# Mesafe tabanlÄ± bir algoritma olduÄŸu iÃ§in Ã¶lÃ§eklendirme gereklidir.

# Naive Bayes sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±
from sklearn.naive_bayes import GaussianNB 
# GaussianNB: OlasÄ±lÄ±k tabanlÄ± bir algoritmadÄ±r ve Bayes teoremini kullanarak tahmin yapar.
# Ã–zellikle metin madenciliÄŸi ve spam tespitinde kullanÄ±lÄ±r.

# Destek VektÃ¶r Makineleri (SVM) sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±
from sklearn.svm import SVC 
# SVC: Destek vektÃ¶r makineleri, verileri sÄ±nÄ±flar arasÄ±nda en iyi ÅŸekilde ayÄ±ran bir hiper dÃ¼zlem bulmaya Ã§alÄ±ÅŸÄ±r.
# KÃ¼Ã§Ã¼k veri setlerinde yÃ¼ksek performans gÃ¶sterir.

# Topluluk (ensemble) Ã¶ÄŸrenme yÃ¶ntemleri
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier 
# AdaBoostClassifier: ZayÄ±f Ã¶ÄŸrenicileri birleÅŸtirerek gÃ¼Ã§lÃ¼ bir sÄ±nÄ±flandÄ±rÄ±cÄ± oluÅŸturur.
# GradientBoostingClassifier: Hata oranÄ±nÄ± azaltarak daha iyi tahminler yapmak iÃ§in gradyan artÄ±rma yÃ¶ntemini kullanÄ±r.
# RandomForestClassifier: Birden fazla karar aÄŸacÄ± oluÅŸturarak daha kararlÄ± ve gÃ¼Ã§lÃ¼ bir model Ã¼retir.

# UyarÄ±larÄ± gÃ¶rmezden gelmek iÃ§in
import warnings 
warnings.filterwarnings("ignore") 
# Ã‡alÄ±ÅŸma sÄ±rasÄ±nda oluÅŸabilecek gereksiz uyarÄ±larÄ± gizleyerek kodun daha temiz Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.



df = pd.read_csv("diabetes.csv") 
 # pandas kÃ¼tÃ¼phanesinin 'read_csv' fonksiyonu ile "diabetes.csv" dosyasÄ±ndaki verileri okur ve df deÄŸiÅŸkenine yÃ¼kler. 
 # #Bu, verileri bir DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. DataFrame, tablo ÅŸeklinde dÃ¼zenlenmiÅŸ verilerin saklanmasÄ±nÄ± saÄŸlar.
df_name = df.columns  # df veri Ã§erÃ§evesinin (DataFrame) sÃ¼tun adlarÄ±nÄ± alÄ±r. 'df.columns' komutu, 
#veri Ã§erÃ§evesindeki tÃ¼m sÃ¼tun isimlerini dÃ¶ndÃ¼rÃ¼r. Bu, hangi verilerin hangi sÃ¼tunlarda olduÄŸunu gÃ¶rmek iÃ§in kullanÄ±lÄ±r.
df.info()  # dfsayÄ±sÄ±nÄ± gÃ¶sterir. veri Ã§erÃ§evesi hakkÄ±nda genel bir bilgi verir. Bu komut, veri setindeki satÄ±r sayÄ±sÄ±nÄ±, sÃ¼tun sayÄ±sÄ±nÄ±, her sÃ¼tunun 
#veri tiplerini (Ã¶rneÄŸin int64, float64) ve eksik deÄŸerlerin 
df.describe()  # df veri Ã§erÃ§evesinin sayÄ±sal sÃ¼tunlarÄ±nÄ±n temel istatistiksel Ã¶zetini verir. Ã–zet, ortalama, standart sapma, minimum ve maksimum deÄŸerler ile 
#Ã§eyrek dilimlere (Q1, Q2, Q3) iliÅŸkin bilgileri iÃ§erir.
describe = df.describe()  # 'df.describe()' fonksiyonunun Ã§Ä±ktÄ±sÄ±nÄ± 'describe' deÄŸiÅŸkenine atar. Bu, daha sonra bu Ã¶zet istatistikleri Ã¼zerinde iÅŸlem yapabilmek iÃ§in
#kullanÄ±lÄ±r.
sns.pairplot(df, hue="Outcome")  # Seaborn kÃ¼tÃ¼phanesinin 'pairplot' fonksiyonunu kullanarak, veri Ã§erÃ§evesindeki
#tÃ¼m sayÄ±sal sÃ¼tunlar arasÄ±ndaki iliÅŸkileri gÃ¶rselleÅŸtirir. 'hue="Outcome"' parametresi, "Outcome" sÃ¼tununa gÃ¶re verileri renklendirir 
#ve diyabetli ve diyabetsiz kiÅŸiler arasÄ±ndaki farklarÄ± gÃ¶rselleÅŸtirir.
plt.show()  # matplotlib kÃ¼tÃ¼phanesinin 'show()' fonksiyonunu Ã§aÄŸÄ±rarak, daha Ã¶nce oluÅŸturulan grafikleri ekranda gÃ¶sterir. 
#Bu komut, gÃ¶rselleÅŸtirilen verilerin ekranda gÃ¶rÃ¼ntÃ¼lenmesini saÄŸlar.

# plot_correlation_heatmap fonksiyonu, verilen veri Ã§erÃ§evesi (df) Ã¼zerinde korelasyon Ä±sÄ± haritasÄ± (heatmap) Ã§izer
def plot_correlation_heatmap(df):
    
    # Veri Ã§erÃ§evesindeki sayÄ±sal sÃ¼tunlar arasÄ±ndaki korelasyon matrisini hesapla
    corr_matrix = df.corr()
    
    # Grafik boyutunu ayarla: geniÅŸlik 10, yÃ¼kseklik 8
    plt.figure(figsize=(10, 8))
    
    # Korelasyon matrisini Ä±sÄ± haritasÄ± olarak Ã§iz
    # annot=True: HÃ¼crelerde korelasyon deÄŸerlerinin yazdÄ±rÄ±lmasÄ±nÄ± saÄŸlar
    # cmap="coolwarm": SoÄŸuk ve sÄ±cak renk paleti kullanÄ±larak korelasyon deÄŸerleri renklerle gÃ¶sterilir
    # fmt=".2f": SayÄ±lar iki ondalÄ±k basamaÄŸa yuvarlanarak gÃ¶sterilecektir
    # linewidths=0.5: HÃ¼creler arasÄ±ndaki Ã§izgilerin kalÄ±nlÄ±ÄŸÄ± 0.5 olacak ÅŸekilde ayarlanÄ±r
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
    
    # GrafiÄŸe baÅŸlÄ±k ekle
    plt.title("Correlation of Features")
    
    # GrafiÄŸi gÃ¶ster
    plt.show()

# Verilen veri Ã§erÃ§evesi (df) ile fonksiyonu Ã§aÄŸÄ±r
plot_correlation_heatmap(df)

import pandas as pd

def detect_outliers_iqr(df):
    outlier_indices = []  # AykÄ±rÄ± deÄŸerlerin indekslerini saklamak iÃ§in boÅŸ bir liste oluÅŸturuyoruz.
    outliers_df = pd.DataFrame()  # AykÄ±rÄ± deÄŸerleri iÃ§erecek boÅŸ bir DataFrame oluÅŸturuyoruz.

    # YalnÄ±zca sayÄ±sal (float64 ve int64) sÃ¼tunlarÄ± seÃ§iyoruz.
    for col in df.select_dtypes(include=["float64", "int64"]).columns:
        Q1 = df[col].quantile(0.23)  # 1. Ã§eyrek (Q1)
        Q3 = df[col].quantile(0.75)  # 3. Ã§eyrek (Q3)
        IQR = Q3 - Q1  # Ã‡eyrekler arasÄ± aÃ§Ä±klÄ±k

        lower_bound = Q1 - 2.5 * IQR  # Alt sÄ±nÄ±r
        upper_bound = Q3 + 2.5 * IQR  # Ãœst sÄ±nÄ±r

        # AykÄ±rÄ± deÄŸerleri seÃ§
        outliers_in_col = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

        outlier_indices.extend(outliers_in_col.index)  # Ä°ndeksleri ekle
        outliers_df = pd.concat([outliers_df, outliers_in_col], axis=0)  # AykÄ±rÄ±larÄ± birleÅŸtir

    # Tekrar eden indeksleri kaldÄ±r
    outlier_indices = list(set(outlier_indices))
    outliers_df = outliers_df.drop_duplicates()

    return outliers_df, outlier_indices  


# Ã–rnek bir DataFrame'in aykÄ±rÄ± deÄŸerlerini tespit etmek iÃ§in fonksiyonu Ã§aÄŸÄ±rÄ±yoruz.
outliers_df, outlier_indices = detect_outliers_iqr(df)

# KaÃ§ tane aykÄ±rÄ± deÄŸerin silindiÄŸini ekrana yazdÄ±rÄ±yoruz.
print(f"Toplam {len(outlier_indices)} aykÄ±rÄ± deÄŸer silindi.")

# AykÄ±rÄ± deÄŸerleri veri setinden kaldÄ±rÄ±yoruz ve indeksleri sÄ±fÄ±rlÄ±yoruz.
df_cleaned = df.drop(outlier_indices).reset_index(drop=True)


# Train test split
x = df_cleaned.drop(["Outcome"], axis=1)
y = df_cleaned["Outcome"]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# Standartizasyon
scaler = StandardScaler()

x_train_scaled = scaler.fit_transform(x_train)  # fit_transform kullanÄ±ldÄ±
x_test_scaled = scaler.transform(x_test)  # transform kullanÄ±ldÄ±


# Model listesi oluÅŸturan fonksiyon
def getBaseModel():
    """FarklÄ± makine Ã¶ÄŸrenmesi modellerini bir liste olarak dÃ¶ndÃ¼rÃ¼r."""
    baseModels = []
    baseModels.append(("LR", LogisticRegression()))  # Lojistik Regresyon
    baseModels.append(("DT", DecisionTreeClassifier()))  # Karar AÄŸacÄ±
    baseModels.append(("KNN", KNeighborsClassifier()))  # K-En YakÄ±n KomÅŸu (Hata dÃ¼zeltildi: () eklendi)
    baseModels.append(("NB", GaussianNB()))  # Naive Bayes
    baseModels.append(("SVM", SVC()))  # Destek VektÃ¶r Makineleri
    baseModels.append(("AdoB", AdaBoostClassifier()))  # AdaBoost
    baseModels.append(("GBM", GradientBoostingClassifier()))  # Gradient Boosting
    baseModels.append(("RF", RandomForestClassifier()))  # Rastgele Orman

    return baseModels  # Model listesini dÃ¶ndÃ¼rÃ¼r

# Modelleri eÄŸitme ve Ã§apraz doÄŸrulama ile deÄŸerlendirme fonksiyonu
def baseModelsTraining(x_train, y_train, models):
    """
    Verilen eÄŸitim verileri ile modelleri eÄŸitir ve Ã§apraz doÄŸrulama uygular.
    10 katlÄ± Ã§apraz doÄŸrulama ile modellerin doÄŸruluÄŸunu hesaplar.
    """
    results = []  # SonuÃ§larÄ± saklamak iÃ§in liste
    names = []  # Model isimlerini saklamak iÃ§in liste

    for name, model in models:
        kf = KFold(n_splits=10, shuffle=True, random_state=42)  # 10 katlÄ± Ã§apraz doÄŸrulama (shuffle=True eklendi)
        cv_results = cross_val_score(model, x_train, y_train, cv=kf, scoring="accuracy")  # Modelin doÄŸruluÄŸunu hesapla
        results.append(cv_results)  # SonuÃ§larÄ± listeye ekle
        names.append(name)  # Model ismini listeye ekle
        print(f"{name}: accuracy: {cv_results.mean():.4f}, std: {cv_results.std():.4f}")  # Ortalama ve standart sapmayÄ± yazdÄ±r

    return names, results  # Model isimleri ve doÄŸruluk sonuÃ§larÄ±nÄ± dÃ¶ndÃ¼r

# Boxplot ile model baÅŸarÄ±larÄ±nÄ± gÃ¶rselleÅŸtirme fonksiyonu
def plot_Box(names, results):
    """
    Modellerin doÄŸruluk deÄŸerlerini kutu grafiÄŸi (boxplot) ile gÃ¶rselleÅŸtirir.
    """
    df = pd.DataFrame({names[i]: results[i] for i in range(len(names))})  # Model sonuÃ§larÄ±nÄ± DataFrame'e Ã§evir
    plt.figure(figsize=(12, 8))  # Grafik boyutunu belirle
    sns.boxplot(data=df)  # Seaborn ile kutu grafiÄŸi Ã§iz
    plt.title("Model Accuracy")  # BaÅŸlÄ±k ekle
    plt.ylabel("Accuracy")
    plt.show()  # GrafiÄŸi gÃ¶ster

# Model eÄŸitim ve deÄŸerlendirme iÅŸlemi
models = getBaseModel()  # Model listesini al
names, results = baseModelsTraining(x_train, y_train, models)  # Modelleri eÄŸit ve deÄŸerlendir

# SonuÃ§larÄ± gÃ¶rselleÅŸtir
plot_Box(names, results)


#hyperparameter tuning
# Hiperparametre ayarlamasÄ± iÃ§in kullanÄ±lacak parametreleri belirliyoruz

param_grid = { # Bu satÄ±r, bir Python sÃ¶zlÃ¼ÄŸÃ¼ (dictionary) oluÅŸturuyor. 
    #Bu sÃ¶zlÃ¼k, makine Ã¶ÄŸrenmesi modelini optimize etmek iÃ§in kullanÄ±lacak 
   # hiperparametrelerin isimlerini ve bu parametreler iÃ§in denenecek olasÄ± deÄŸerleri iÃ§eriyor.
    "criterion": ["gini", "entropy"],  
    #ğŸ”¹ criterion: Karar aÄŸacÄ± modelinin hangi Ã¶lÃ§Ã¼tÃ¼ kullanarak bÃ¶lÃ¼nme (split) yapacaÄŸÄ±nÄ± belirleyen parametredir.
#"gini": Gini indeksini kullanarak dÃ¼ÄŸÃ¼m bÃ¶lme iÅŸlemini yapar. DÃ¼ÅŸÃ¼k Gini deÄŸeri daha saf (homojen) veri kÃ¼meleri oluÅŸturur.
#"entropy": Entropi Ã¶lÃ§Ã¼tÃ¼nÃ¼ kullanarak dÃ¼ÄŸÃ¼m bÃ¶lme iÅŸlemini yapar. Bilgi kazancÄ±na dayanarak bÃ¶lmeyi belirler.
#ğŸ”¹ Bu parametre, modelin neye gÃ¶re dallanacaÄŸÄ±nÄ± kontrol eder.
    "max_depth": [10, 20, 30, 40, 50],  
     #max_depth: Karar aÄŸacÄ±nÄ±n maksimum derinliÄŸini belirleyen parametredir.
#Daha dÃ¼ÅŸÃ¼k deÄŸerler (Ã¶rneÄŸin, 10 veya 20) modeli basitleÅŸtirerek aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) engelleyebilir.
#Daha yÃ¼ksek deÄŸerler (Ã¶rneÄŸin, 40 veya 50) aÄŸacÄ± daha karmaÅŸÄ±k hale getirerek veriye daha iyi uyum saÄŸlayabilir ama aÅŸÄ±rÄ± Ã¶ÄŸrenme riski taÅŸÄ±r.
#ğŸ”¹ Bu parametre, aÄŸacÄ±n ne kadar derine inebileceÄŸini sÄ±nÄ±rlar.
    "min_samples_split": [2, 5, 10],  
     #min_samples_split: Bir dÃ¼ÄŸÃ¼mÃ¼n dallanabilmesi iÃ§in en az kaÃ§ Ã¶rnek (sample) iÃ§ermesi gerektiÄŸini belirler.
#2: EÄŸer dÃ¼ÄŸÃ¼mde en az 2 Ã¶rnek varsa, bÃ¶lÃ¼nebilir. (Daha kÃ¼Ã§Ã¼k ve derin aÄŸaÃ§lar oluÅŸur.)
#5: Bir dÃ¼ÄŸÃ¼mde en az 5 Ã¶rnek olduÄŸunda bÃ¶lÃ¼nebilir.
#10: En az 10 Ã¶rnek varsa bÃ¶lÃ¼nebilir. (Daha bÃ¼yÃ¼k ve daha az dallanmÄ±ÅŸ bir aÄŸaÃ§ oluÅŸur.)
#ğŸ”¹ Bu parametre, aÄŸacÄ±n aÅŸÄ±rÄ± detaycÄ± olmasÄ±nÄ± engellemek iÃ§in kullanÄ±lÄ±r.
    "min_samples_leaf": [1, 2, 4] 
    #ğŸ”¹ min_samples_leaf: Bir yaprak dÃ¼ÄŸÃ¼mÃ¼nde (leaf node) bulunmasÄ± gereken en az Ã¶rnek sayÄ±sÄ±nÄ± belirler.
#1: Yaprak dÃ¼ÄŸÃ¼mlerinde tek bir Ã¶rnek olabilir. (AÄŸaÃ§ daha detaylÄ± Ã¶ÄŸrenir.)
#2: Yaprak dÃ¼ÄŸÃ¼mlerinde en az 2 Ã¶rnek olmalÄ±dÄ±r.
#4: Yaprak dÃ¼ÄŸÃ¼mlerinde en az 4 Ã¶rnek olmalÄ±dÄ±r. (AÄŸaÃ§ daha az dallanÄ±r, genelleme gÃ¼cÃ¼ artabilir.)
#ğŸ”¹ Bu parametre, aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nlemek iÃ§in yaprak dÃ¼ÄŸÃ¼mlerin minimum bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ belirler.
}


# Karar aÄŸacÄ± modelini oluÅŸturuyoruz
dt = DecisionTreeClassifier()

# Grid Search CV (GridSearchCV), en iyi parametre kombinasyonunu belirlemek iÃ§in Ã§apraz doÄŸrulama uygular
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring="accuracy")
#GridSearchCV, verilen parametrelerin tÃ¼m kombinasyonlarÄ±nÄ± deneyerek en iyi sonucu veren parametreleri seÃ§meye Ã§alÄ±ÅŸÄ±r.
#Veri seti 5 eÅŸit parÃ§aya bÃ¶lÃ¼nÃ¼r. Model 5 kez eÄŸitilip test edilir ve her seferinde farklÄ± bir kÄ±sÄ±m test iÃ§in kullanÄ±lÄ±r.
#Accuracy (DoÄŸruluk), doÄŸru sÄ±nÄ±flandÄ±rÄ±lan Ã¶rneklerin toplam Ã¶rneÄŸe oranÄ±dÄ±r.
# Modeli eÄŸitim verileri ile eÄŸitiyoruz


grid_search.fit(x_train, y_train)

# En iyi parametreleri ekrana yazdÄ±rÄ±yoruz
print("En iyi parametreler:", grid_search.best_params_)

# En iyi bulunan modeli alÄ±yoruz
best_dt_model = grid_search.best_estimator_

# Test verileri ile tahmin yapÄ±yoruz
y_pred = best_dt_model.predict(x_test)
#ğŸ”¹ y_pred (Tahmin Edilen Etiketler - Predicted Labels):
# Modelin x_test verileri iÃ§in tahmin ettiÄŸi sonuÃ§larÄ± iÃ§erir.

# Confusion Matrix'i ekrana yazdÄ±rÄ±yoruz
print("Confusion Matrix")
print(confusion_matrix(y_test, y_pred))

# SÄ±nÄ±flandÄ±rma raporunu ekrana yazdÄ±rÄ±yoruz
print("Classification Report")
print(classification_report(y_test, y_pred))

# Model testing with real data


# Burada aÅŸaÄŸÄ±daki adÄ±mlar gerÃ§ekleÅŸtirilecek:
# 1. Veriyi iÃ§e aktarma ve KeÅŸifsel Veri Analizi (EDA)
# 2. AykÄ±rÄ± deÄŸer (outlier) tespiti
# 3. Veri setinin eÄŸitim ve test kÃ¼melerine ayrÄ±lmasÄ±
# 4. Verinin standartizasyonu
# 5. Model eÄŸitimi ve deÄŸerlendirmesi
# 6. Hiperparametre ayarÄ± (hyperparameter tuning)
# 7. Modelin gerÃ§ek veri ile test edilmesi

